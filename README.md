# Assignment #3 - Transformer and BERT


Deep Learning / Spring 1401, Shahid Beheshti University



![BERT INPUTS](https://res.cloudinary.com/m3hrdadfi/image/upload/v1595158991/kaggle/bert_inputs_w8rith.png)

As you may know, the BERT model input is a combination of 3 embeddings.
- Token embeddings: WordPiece token vocabulary (WordPiece is another word segmentation algorithm, similar to BPE)
- Segment embeddings: for pair sentences [A-B] marked as $E_A$ or $E_B$ mean that it belongs to the first sentence or the second one.
- Position embeddings: specify the position of words in a sentence


**Setup**
Click on Download [problems] to obtain the assignment jupyter notebook.
Go to https://colab.research.google.com/.
Switch to Upload tab, choose assignment_4.ipynb and click upload.
Now Youâ€™re ready to go.
